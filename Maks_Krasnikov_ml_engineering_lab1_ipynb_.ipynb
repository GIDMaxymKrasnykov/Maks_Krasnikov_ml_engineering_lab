{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GIDMaxymKrasnykov/Maks_Krasnikov_ml_engineering_lab/blob/main/Maks_Krasnikov_ml_engineering_lab1_ipynb_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Підготовка даних**"
      ],
      "metadata": {
        "id": "tDvslSs2UACr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Configurations\n",
        "CONFIG = {\n",
        "    \"data_dir\": \"./data\",\n",
        "    \"batch_size\": 64,\n",
        "    \"num_workers\": 4,\n",
        "    \"validation_split\": 0.1,\n",
        "    \"test_split\": 0.1,\n",
        "    \"seed\": 42,\n",
        "}\n",
        "\n",
        "# Ensure reproducibility\n",
        "torch.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "# Data Preparation\n",
        "def prepare_data():\n",
        "    \"\"\"Download, preprocess, and split the dataset.\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Download CIFAR-10 dataset\n",
        "    dataset = torchvision.datasets.CIFAR10(root=CONFIG[\"data_dir\"], train=True, download=True, transform=transform)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root=CONFIG[\"data_dir\"], train=False, download=True, transform=transform)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    total_size = len(dataset)\n",
        "    val_size = int(CONFIG[\"validation_split\"] * total_size)\n",
        "    train_size = total_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=CONFIG[\"num_workers\"])\n",
        "    val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"num_workers\"])\n",
        "    test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"num_workers\"])\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# Prepare the data\n",
        "if __name__ == \"__main__\":\n",
        "    train_loader, val_loader, test_loader = prepare_data()\n",
        "    print(\"Data preparation complete.\\n\")\n",
        "    print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "    print(f\"Test samples: {len(test_loader.dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grgHVy8-THuD",
        "outputId": "c9df0658-2cc5-4e63-96fd-de2f73d490db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:10<00:00, 16.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation complete.\n",
            "\n",
            "Training samples: 45000\n",
            "Validation samples: 5000\n",
            "Test samples: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Створення простої нейронної мережі, реалізації циклів навчання та валідації, а також оновлення аугментацію даних**"
      ],
      "metadata": {
        "id": "g7_V-ye0T-JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Configurations\n",
        "CONFIG = {\n",
        "    \"data_dir\": \"./data\",\n",
        "    \"batch_size\": 64,\n",
        "    \"num_workers\": 4,\n",
        "    \"validation_split\": 0.1,\n",
        "    \"test_split\": 0.1,\n",
        "    \"seed\": 42,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"num_epochs\": 10\n",
        "}\n",
        "\n",
        "# Ensure reproducibility\n",
        "torch.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "# Data Preparation\n",
        "def prepare_data():\n",
        "    \"\"\"Download, preprocess, and split the dataset.\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Download CIFAR-10 dataset\n",
        "    dataset = torchvision.datasets.CIFAR10(root=CONFIG[\"data_dir\"], train=True, download=True, transform=transform)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root=CONFIG[\"data_dir\"], train=False, download=True, transform=transform)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    total_size = len(dataset)\n",
        "    val_size = int(CONFIG[\"validation_split\"] * total_size)\n",
        "    train_size = total_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=CONFIG[\"num_workers\"])\n",
        "    val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"num_workers\"])\n",
        "    test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"num_workers\"])\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# Define Simple Neural Network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 32 * 3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Training Loop\n",
        "def train_model(train_loader, val_loader):\n",
        "    \"\"\"Train the neural network.\"\"\"\n",
        "    model = SimpleNN()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
        "\n",
        "    for epoch in range(CONFIG[\"num_epochs\"]):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{CONFIG['num_epochs']}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "        # Validation step\n",
        "        validate_model(model, val_loader)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Validation Loop\n",
        "def validate_model(model, val_loader):\n",
        "    \"\"\"Validate the neural network.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Main Workflow\n",
        "if __name__ == \"__main__\":\n",
        "    train_loader, val_loader, test_loader = prepare_data()\n",
        "    print(\"Data preparation complete.\\n\")\n",
        "    print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "    print(f\"Test samples: {len(test_loader.dataset)}\")\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trained_model = train_model(train_loader, val_loader)\n",
        "    print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKtE3YBSUq0i",
        "outputId": "0e107317-c6f3-44d6-82d3-056624ff7533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation complete.\n",
            "\n",
            "Training samples: 45000\n",
            "Validation samples: 5000\n",
            "Test samples: 10000\n",
            "Starting training...\n",
            "Epoch [1/10], Loss: 1.8049\n",
            "Validation Accuracy: 37.86%\n",
            "Epoch [2/10], Loss: 1.6426\n",
            "Validation Accuracy: 42.80%\n",
            "Epoch [3/10], Loss: 1.5806\n",
            "Validation Accuracy: 45.00%\n",
            "Epoch [4/10], Loss: 1.5356\n",
            "Validation Accuracy: 45.48%\n",
            "Epoch [5/10], Loss: 1.5086\n",
            "Validation Accuracy: 45.76%\n",
            "Epoch [6/10], Loss: 1.4789\n",
            "Validation Accuracy: 46.64%\n",
            "Epoch [7/10], Loss: 1.4607\n",
            "Validation Accuracy: 47.00%\n",
            "Epoch [8/10], Loss: 1.4439\n",
            "Validation Accuracy: 47.72%\n",
            "Epoch [9/10], Loss: 1.4287\n",
            "Validation Accuracy: 47.62%\n",
            "Epoch [10/10], Loss: 1.4207\n",
            "Validation Accuracy: 48.66%\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Тестування моделі та обчислення додаткових метрик (precision, recall, F1-score). Тепер модель може бути протестована після навчання, а результати будуть представлені у вигляді метрик.**"
      ],
      "metadata": {
        "id": "skvs6c8zVa5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Configurations\n",
        "CONFIG = {\n",
        "    \"data_dir\": \"./data\",\n",
        "    \"batch_size\": 64,\n",
        "    \"num_workers\": 4,\n",
        "    \"validation_split\": 0.1,\n",
        "    \"test_split\": 0.1,\n",
        "    \"seed\": 42,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"num_epochs\": 10\n",
        "}\n",
        "\n",
        "# Ensure reproducibility\n",
        "torch.manual_seed(CONFIG[\"seed\"])\n",
        "\n",
        "# Data Preparation\n",
        "def prepare_data():\n",
        "    \"\"\"Download, preprocess, and split the dataset.\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    # Download CIFAR-10 dataset\n",
        "    dataset = torchvision.datasets.CIFAR10(root=CONFIG[\"data_dir\"], train=True, download=True, transform=transform)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root=CONFIG[\"data_dir\"], train=False, download=True, transform=transform)\n",
        "\n",
        "    # Calculate split sizes\n",
        "    total_size = len(dataset)\n",
        "    val_size = int(CONFIG[\"validation_split\"] * total_size)\n",
        "    train_size = total_size - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=CONFIG[\"num_workers\"])\n",
        "    val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"num_workers\"])\n",
        "    test_loader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"num_workers\"])\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# Define Simple Neural Network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 32 * 3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Training Loop\n",
        "def train_model(train_loader, val_loader):\n",
        "    \"\"\"Train the neural network.\"\"\"\n",
        "    model = SimpleNN()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
        "\n",
        "    for epoch in range(CONFIG[\"num_epochs\"]):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{CONFIG['num_epochs']}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "        # Validation step\n",
        "        validate_model(model, val_loader)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Validation Loop\n",
        "def validate_model(model, val_loader):\n",
        "    \"\"\"Validate the neural network.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
        "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Test Model\n",
        "def test_model(model, test_loader):\n",
        "    \"\"\"Test the trained model and report metrics.\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
        "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Main Workflow\n",
        "if __name__ == \"__main__\":\n",
        "    train_loader, val_loader, test_loader = prepare_data()\n",
        "    print(\"Data preparation complete.\\n\")\n",
        "    print(f\"Training samples: {len(train_loader.dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
        "    print(f\"Test samples: {len(test_loader.dataset)}\")\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trained_model = train_model(train_loader, val_loader)\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "    print(\"Starting testing...\")\n",
        "    test_model(trained_model, test_loader)\n",
        "    print(\"Testing complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2p9afEeVhdj",
        "outputId": "f290b58b-83a9-4702-c077-e9dcf4254514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation complete.\n",
            "\n",
            "Training samples: 45000\n",
            "Validation samples: 5000\n",
            "Test samples: 10000\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.8049\n",
            "Validation Accuracy: 37.86%\n",
            "Precision: 0.3940, Recall: 0.3786, F1-Score: 0.3677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 1.6426\n",
            "Validation Accuracy: 42.80%\n",
            "Precision: 0.4333, Recall: 0.4280, F1-Score: 0.4186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 1.5806\n",
            "Validation Accuracy: 45.00%\n",
            "Precision: 0.4571, Recall: 0.4500, F1-Score: 0.4471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 1.5356\n",
            "Validation Accuracy: 45.48%\n",
            "Precision: 0.4568, Recall: 0.4548, F1-Score: 0.4517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 1.5086\n",
            "Validation Accuracy: 45.76%\n",
            "Precision: 0.4575, Recall: 0.4576, F1-Score: 0.4492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 1.4789\n",
            "Validation Accuracy: 46.64%\n",
            "Precision: 0.4709, Recall: 0.4664, F1-Score: 0.4613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 1.4607\n",
            "Validation Accuracy: 47.00%\n",
            "Precision: 0.4696, Recall: 0.4700, F1-Score: 0.4610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 1.4439\n",
            "Validation Accuracy: 47.72%\n",
            "Precision: 0.4896, Recall: 0.4772, F1-Score: 0.4777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 1.4287\n",
            "Validation Accuracy: 47.62%\n",
            "Precision: 0.4707, Recall: 0.4762, F1-Score: 0.4619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 1.4207\n",
            "Validation Accuracy: 48.66%\n",
            "Precision: 0.4834, Recall: 0.4866, F1-Score: 0.4763\n",
            "Training complete.\n",
            "Starting testing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 48.64%\n",
            "Precision: 0.4826, Recall: 0.4864, F1-Score: 0.4745\n",
            "Testing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Автоматичне створення графіків**"
      ],
      "metadata": {
        "id": "nHzF8_4OY8jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metrics(metrics, save_path=\"metrics.png\"):\n",
        "    epochs = range(1, len(metrics[\"accuracy\"]) + 1)\n",
        "    plt.plot(epochs, metrics[\"accuracy\"], label=\"Accuracy\")\n",
        "    plt.plot(epochs, metrics[\"f1\"], label=\"F1-Score\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Metric Value\")\n",
        "    plt.legend()\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xOhN1-vhY-ui"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}